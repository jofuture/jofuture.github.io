학습목표 : ANOVA test, Chi squared test에 대한 개념확립과 베이지안의 의의를 생각해본다.

학습정리 기준 :

1\. **ANOVA test**가 가지는 의의와 **개념설명**, 가설검정에서 활용

2\. Non parametric methods 중 **Chi squared test**에 대한 개념설명

3\. **ANOVA test, Chi squared test 실습** → python활용 (@강의자료 참고)

4\. Bayesian Inference을 위한 **베이즈 정리 개념 확립**, **통계학에서 베이즈 정리가 가지는 의의** 생각해볼 것

---

앞전 내용

t-test

모집단의 표준편차가 알려지지 않았을 때, 정규분포의 모집단에서 모은 샘플(표본)의 평균값에 대한 가설검정 방법

두 개의 샘플이 같은지 다른지 알아보고자 할때 하는 것

그룹이 한 개 더 있다면 어떻게 될까 ?

t-test를 여러번한것 -> multiple t-test

-   비교할 집단이 세 개 이상인 경우  
    t-test를 세 번하면 될 것 같았으나 할 수 없음 -> 1종오류에 빠짐  
    따라서, 우리는 새로운 방법을 배워야 함
-   그 새로운 통계 방법이 바로 One-way ANOVA 임  
    One way는 독립변수가 하나라는 뜻   
    ANOVA는 Analysis of Variance 의 약자  
    한국말로는 분산분석이라고 한다.
-   이름이 왜 평균분석이 아니고 왜 분산분석일까?  
    **A/B검정 말고, 여러 그룹 예를 들어 A-B-C-D의 수치 데이터들을 서로 비교한다고 가정해볼때**  
    **여러 그룹 간의 통계적으로 유의미한 차이를 검정하는 통계절 절차를**   
    **분산분석 줄여서 ANOVA라고 한다.**  
      
    

## **ANOVA 전에 알아야 할 것들**

ANOVA는 평균을 비교하는 것 이상을 수행한다.

다양한 그룹의 평균값이 다른 것처럼 보이지만 이는 독립 변수가 종속 변수에 미치는 영향이 아니라 샘플링 오류 때문일 수 있다. 샘플링 오류로 인한 경우 그룹 평균 간의 차이는 의미가 없다. ANOVA를 사용하여 평균값의 차이가 통계적으로 유의한지 확인 할 수 있다.

또한 독립변수가 종속변수에 영향을 미치는지를 간접적으로 보여준다.

<img src="https://blog.kakaocdn.net/dn/ccy3QI/btrhIpbXWZy/OubVguMw1UiANDb52QkrG1/img.jpg">

-   **독립변수 (Independent variable)**  
-   
    연구자/조사자가 의도적으로 변화시키는 변수  
    
    다른 말로 예측변수 혹은 설명변수  
    
    여기서 독립은 논리적 관계에서의 독립을 의미한다.  
    
      - 주로 인과관계를 많이 연구/조사하므로 이때 이 인과관계에서 독립적인 위치는  
        결과보다는 원인일 것이다.  
        
      - 그래서 인과관계에서 원인이 되는 변수가 독립변수가 된다.  
      - 
    오해하면 안되는 것은 독립변수가 연구자/조사자가 의도적으로 변화시킬 수 있다고 하여 
    
    마음대로 해도 된다는 의미는 아니다.  
      
    
-   **종속변수 (Dependent variable)**  
-   
    연구자/조사자가 독립변수의 변화에 따라 어떻게 변하는지 알고 싶은 변수  
    
    다른 말로 반응변수 혹은 결과변수  
    
    여기서 종속은 논리적 관계에서의 종속을 의미한다.  
    
      - 주로 인과관계를 많이 연구/조사하므로 이때 이 인과관계에서 종속적인 위치는
        
        원인보다는 결과일 것이다.  
        
      - 그래서 인과관계에서 결과가 되는 변수가 종속변수가 된다.  
      
    
-   **통제변수 (Control variable)**  
    기본적으로는 독립변수와 동일하나  
    연구/조사의 주된 관심사가 되는 변수가 아닌 경우  
    통제변수를 사용하는 이유  
       - 고객만족 ↑→ 재방문율↑  
       - 이때, 고객만족이 독립변수, 재방문율이 종속변수  
       - 그런데, 재방문율에 영향을 미치는 변수가 오직 고객만족 한 개 일리는 없음  
       - 따라서, 재방문율에 영향을 미칠 만한 다른 중요한 변수를 같이 감안해야 함  
       - 다른 중요한 변수를 감안하고서도 고객만족이 중요한 역할을 한다면, 고객만족은  
         정말 중요한 변수임  
             - 통제변수를 한개도 감안하지 않는 경우 : Model Misspecification 발생

#### **ANOVA와 변수**

-   **One-way ANOVA에 사용되는 변수**  
    종속변수 : 연속형(Continuous)변수만 가능  
    독립변수 : 이산형/범주형(Discrete/Categorical) 변수만 가능  
      
    
-   예시 (1)  
    신종플루 신약을 개발한 어느 제약회사에서 신종플루 감염자를  
    대상으로 신약의 효과를 측정하고자 한다  
    \- 종속변수 : 신약을 먹은 후 완치 될 때까지 걸린 날짜  
    \- 독립변수 (3 레벨- 3가지 그룹)  
       - 새로 개발된 신약  
       - 기존의 독감 약  
       - 플라시보
-   예시 (2)  
    어느 인터넷 ISP 기업이 고객의 총 지불금액이 고객들의 지불 방법에 따라  
    차이가 있는지 알고 싶어한다  
    \- 종속변수 : 고객의 총 지불금액 (Total Charges)  
    \- 독립변수 (4 레벨- 4가지 그룹)  
       - 은행계좌 자동이체 (Bank Transfer)  
       - 신용카드 (Credit Card)  
       - 전자수표 (Electronic Check)  
       - 종이수표 (Mailed Check)

#### **One-way ANOVA**  

-   One-way는 독립변수가 한개라는 뜻 여기서 독립변수는 Main effect(주효과)가 있다고 함
-   z-test 에는 z-value (z값)이 있었고 t-test 에는 t-value (t값)이 있다.
-   **ANOVA 에는 F-value(F값) 이 있다.**  
    물론 이와 함께 F분포도 있다.  
    앞의 다른 테스트와 마찬가지로 F-값을 구해서 F분포를 확인한다.
-   왜 F-value 이냐? F값이란?  
    이 비율은 그룹 내 분산과 그룹 간 분산 간의 차이를 보여준다.  
    그래서 우리는 이것을 분산분석이라 부른다.  
      
    

#### **Two-way ANOVA**

-   Two-way는 독립변수가 두 개라는 뜻. 즉, Main effect가 2개임  
    
-   독립변수가 2개 + a 인 ANOVA  
    그렇다면 여기서 a 는 무엇인가?  
      -  Two-way ANOVA 의 a 는 interaction (상호작용/교호작용) 임  
      -  Interaction 이 여기서 등장하는 이유는?
-   Interaction 이란 무엇인가?  
    한 독립변수의 Main effect가 다른 독립변수의 level (=group)에 따라   
    원래의 linear relationship이 non-linear 하게 변하는 경우
-   Linear relationship 부터 알아보자  
    Linear relationship  이란 선형 관계라고 한다.  
    여기서 선형은 직선을 의미한다.  
    즉 독립변수와 종속변수의 관계가 (직)선형관계라고 이미 전제된 것  
    t-test이든 ANOVA이든 이미 사전에(직)선형관계는 전제된 것이다.  
    우리가 하게 될 모든 분석은 일단 선형관계를 전제 한다고 이해하고 시작해야만 한다.
-   Two-way ANOVA 에서는 어떻게 바뀔까?  
    일단 독립변수가 2개이므로 F값이 2개가 되어야 한다.  
    또한 우리는 추가적으로 interaction도 유의한지 아닌지 알아야 한다.  
    따라서 interaction에 대한 F값도 한 개 더 필요하다.
-   그러므로 우리는 총 3개의 F-value 값을 구해야 한다  
    또한 3개의 Between variance가 필요하다 
-   통계적 가설은 몇 개가 필요할까? 3개  
    첫 번째 main effect가 유의할 경우 -> 사후검정 필요  
    두 번째 main effect가 유의할 경우 -> 사후검정 필요  
    Interaction이 유의할 경우 -> 사후검정 필요(?) 가능은 하지만 매우 복잡함  
      - 사후검정 보다는 그래프를 그려보는 것이 더 이해하기 쉬움

#### **ANOVA의 사용조건**

-   t-test와 마찬가지로 관측치가 정규분포를 따라야 한다.
-   등분산 가정을 만족하여야 한다.  
    등분산 가정은 일반적으로 관측치(샘플수)가 똑같은 경우에는 크게 문제가 되지 않는다.  
    샘플 수가 다른 경우 가장 큰 분산이 가장 작은 분산보다 1.5배 이상 크지 않으면 괜찮다.
-   표본이 독립적이어야 한다.  
    비독립표본(Repeated-measure)인 경우에는 사용하지 않는다.

####  **ANOVA의 한계**

ANOVA는 평균만 비교하기 때문에 데이터 세트가 균일하게 분포되어 있다고 가정한다.

데이터가 정규 곡선 형태로 분포 되어 있지 않고 특이치가 있는 경우 ANOVA는 데이터를 해석하는 데 적합한 프로세스가 아니다.

또한, 최소 두 그룹의 평균간에 유의한 차이가 있는지 여부만 알 수 있지만 어떤 쌍에서 평균이 다른지는 설명불가하다.  
(여기에는 Post-hoc 후속검정이 필요하다 )

세분화된 데이터에 대한 요구 사항이 있는 경우 추가 후속 통계 프로세스를 배포하면 평균값이 다른 그룹을

찾는 데 도움이 된다.  마찬가지로 ANOVA는 표준 편차가 그룹간에 동일하거나 유사하다고 가정한다. 표준 편차에 큰 차이가 

있으면 검정의 결론이 정확하지 않을 수 있다.

일반적으로 ANOVA는 다른 통계 방법과 함께 사용된다.

#### **데이터 사이언스에서 ANOVA는 어떻게 사용되나?**

일반적으로 ANOVA는 다른 통계 방법과 함께 사용된다.

머신러닝의 가장 큰 과제 중 하나는 모델을 학습하는 데 사용되는 가장 안정적이고 유용한 기능을 선택하는 것이다.

ANOVA는 모델의 복잡성을 줄이기 위해 입력 변수의 수를 최소화 한다.

독립변수가 목표변수에 영향을 미치는지 확인하는 데 도움이 된다.

대표적인 예 : 이메일 스팸 감지

---

## **카이제곱 검정**

-   지금까지 공부한 분석방법의 공통점  
    종속변수는 Quantitative variable = 연속형 변수  
    독립변수는 Qualitative variable (categorical variable) = 명목 변수
-   그런데, 만약 둘 다 명목척도(Qualitative variable) 라면 ?  
    t-test 와 ANOVA 모두 사용할 수 없음  
    이 때, 사용하는 것이 **교차분석**임  
    영어로 chi-square test라고 함 여기에 chi-square value와 chi-square분포가 있다
-   카이(chi, χ)란 그리스 알파벳 버전으로 표준정규분포를 의미
-   언제 카이제곱 검정을 할까?  
    변수가 명목척도 일 때,  
    자료(데이터)의 값은 개수(count)이어야 함
-   그렇다면 이 검정의 목적은 무엇일까?  
    앞의 t-test 나 ANOVA의 경우 둘/셋 이상의 집단이 같은지 다른지  
    **카이제곱검정은 두 변수간의 연관도를 검정하고자 할 때 사용합니다**
-   카이제곱 검정의 목적은  
    변수가 1 개인 경우 : 변수 내 그룹간의 비율이 같은지 다른지  
    그룹이 2 개인 경우 : Binomail test  
    그룹이 여러 개인 경우 : 카이제곱 검정  
    변수가 두 개인 경우 : 변수 사이의 연관성이 있는지 없는지  
      - ex) 휴대폰 사용과 뇌암인종과 특정 질병

χ2\=∑E(O−E)2​

O \= 관찰 빈도(Observed Frequency), 자료의 값  
E \= 기대 빈도(Expected Frequency), 기대 값  
df(자유도) = 범주의 개수 - 1

#### **One-way Chi-square test  
**

-   One-way ANOVA와 같이 변수가 1개라는 의미
-   한개의 명목척도 변수는 2개 이상의 범주형 데이터를 가진다.
-   일원 카이제곱 검정의 유의성이 의미하는 것은 무엇인가 다르다 정도이다.  
    다르다는 의미는 사전에 정해진 기대값과 다르다는 의미이다.

#### **Two-way Chi-square test**

-   두개의 명목척도인 변수를 대상으로 검정
-   동질성 검증  
    변인의 분포가 이항분포나 정규분포와 동일하다라는 가설을 설정하여 이는 어떤 모집단의 표본이 그 모집단을대표하고 있는지를 검증하는데 사용
-   독립성 검증  
    변인이 두 개 이상일 때 사용되며, 기대빈도는 두변인의 상관관계가 없는 독립성을 기대하는 것을 의미하며 관찰빈도와의 차이를 통해 기대빈도의 진위여부를 확인
-   이원 카이제곱 검정의 목적은 두 변수의 연관성의 확인이다.

---

## **Bayes' Theorem 베이즈 정리 개념**

베이즈 정리는 새로운 사건의 확률을 계산하기 전에 이미 일어난 사건을

고려하는 것을 전제로 하는 베이즈(혹은 베이지안)통계의 근간이라 할 수 있다.

통계학에서 베이즈 정리가 가지는 의의 두 확률변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리이다

베이즈 확률론 해석에 따르면 베이즈 정리는 사전확률 로부터 사후확률을 구할 수 있다.

베이즈 정리는 불확실성 하에서 의사결정문제를 수학적으로 다룰 때 중요하게 이용된다.특히, 정보와 같이 눈에 보이지 않는 무형자산이 지닌 가치를 계산할 때 유용하게 사용된다.전통적인 확률이 연역적 추론에 기반을 두고 있다면 베이즈 정리는 확률임에도 귀납적,경험적인 추론을 사용한다.

출처 - 위키피디아

쉽게 말해 독립사건과 조건부 확률을 떠올리면 된다.

**독립 사건(Independent Events)**

두 사건이 독립적인지 결정하는 건 통계에서 매우 중요하다.

사건이 **독립적**이라는 건 **어떤 사건이 발생**하더라도 그게 **다른 사건이 발생 확률에 영향을 미치지 않는다**는 뜻이다.

예를 들면 동전 던지기 같은 거. 한 번 던져서 앞면이 나왔다고 해서 다시 던질 때 앞면이 나올 확률이 줄어드는 게 아니기 때문이다.

**조건부 확률(conditional probability)**

어떤 사건 B가 발생했을 때 사건 A가 발생할 확률을 의미한다.

만약 두 사건이 서로 독립적이라면 그냥 각각의 사건이 발생할 확률을 곱하면 끝이다. 이렇게.

P(A∩B) = P(A) × P(B)

여기서 P는 확률(probability), ∩ 기호는 “and”의 의미다.

만약 주사위 2개를 던져 모두 6이 나올 확률을 구하면… 각각의 주사위에서 6이 나올 확률은 1/6이고 서로 아무런 영향을 주고 받지 않으니까 그냥 1/6 × 1/6로 구하면 된다. 그런데 이건 지극히 **빈도주의적(Frequentist)** 관점에서의 확률 계산이다. 

주사위를 던져 6이 나올 확률이 구할 때 애초에 6번 던지면 1번 꼴로 나오니까… 그걸 보고 확률을 구한 거다.

**대부분의 기계학습에서 베이즈 정리와 베이즈 분류기를 다루는 이유**

-   베이즈 분류기는 다른 기계학습 방법론들에 비해 상대적으로 알고리즘이 간단함에도 불구하고 현실세계의 많은 문제를 효과적으로 풀 수 있다는 장점이 있다.
-   베이즈 정리를 공부하면서 기계학습을 심도 깊게 이해하기 위해서 꼭 공부해야만하는 확률론에 대한 감을 키울 수 있다.

**베이즈정리 공식**

<img src="https://blog.kakaocdn.net/dn/laJjG/btrhHpXULmQ/cq0Prjd0kdODAMG6zGpg3K/img.png">
<img src="https://blog.kakaocdn.net/dn/kLYg5/btrhH7W5cIz/sqyHqhBYLkyqkpa6bttcsk/img.png">

B라는 조건이 주어졌을 때 A의 확률을 구하는 거다.

그런데 여기서 만약 B를 데이터라고 생각하고, A를 레이블이라고 생각하면 일종의 분류기가 되는 셈이다.

B라는 데이터가 주어졌을 때 A라는 레이블로 분류될 확률을 계산하는 거니까.

예시1)  스팸 필터

예를 들어 메일에 “OO”이라는 단어가 메일에 포함되어 있을 때, 그 메일이 spam일 확률을 구하는 거다. (spam의 반댓말은 ham)

-   “OO”이라는 단어는 ham에서 0.1% 정도 등장난다.
-   “OO”이라는 단어는 spam에서 5% 정도 등장한다.
-   전체 메일의 20%는 spam이다. (80%는 ham이다.)

내가 받은 메일에서 “OO”이라는 단어를 발견했다. 이 메일이 spam일 확률은?

그러면 여기서 **분모**는 “OO”이 포함된 메일을 받을 확률이다. 아래 두 경우를 더해주어야겠지.

-   spam에서 “OO”이 등장할 확률 : 0.2 \* 0.05
-   ham에서 “OO”이 등장할 확률 : 0.8 \* 0.001

그리고 **분자**는 spam에서 “OO”이 등장할 확률(0.05)과 spam을 받을 확률(0.2)을 곱해주면 된다.

구해보니 92.6% 정도 된다.

**베이즈정리의 핵심은 관찰을 통해 새로운 정보를 획득하면 사후 확률(믿음의 정도)을 업데이트 한다는 점**

참고자료 : [https://hleecaster.com/ml-naive-bayes-classifier-concept/](https://hleecaster.com/ml-naive-bayes-classifier-concept/)

            [https://www.tibco.com/ko/reference-center/what-is-analysis-of-variance-anova](https://www.tibco.com/ko/reference-center/what-is-analysis-of-variance-anova)

            [https://hyen4110.tistory.com/17](https://hyen4110.tistory.com/17)

            [https://velog.io/@rsj9987/%EC%B9%B4%EC%9D%B4%EC%A0%9C%EA%B3%B1-%EA%B2%80%EC%A0%95](https://velog.io/@rsj9987/%EC%B9%B4%EC%9D%B4%EC%A0%9C%EA%B3%B1-%EA%B2%80%EC%A0%95)
