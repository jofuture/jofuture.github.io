학습목표 : AI분야에서 선형대수가 어떻게 적용이 되고 있는지 살펴보자2 - 선형대수 활용에 대해서 학습하기

학습정리 기준 :

1\. **linear projection**이란 무엇이며, 데이터 사이언티스트입장에서 어떤 의미가 있는지 확인해볼 것

2\. vector의 **linear transformation**(선형변환, 선형사상이라고도 함)을 살펴보고

   데이터분야에서 어떤 의미를 가지는지 설명해볼 것

3\. **Eigenvector, Eigenvalue** → keyword 중심으로 정의를 내리고, **어디에 활용할 지** 확인해볼 것

4\. **고차원의 문제**, 즉 feature가 너무 많으면 어떤 문제를 일으키는지 확인하고, 해결방법 찾아보기 (n133)

5\. **PCA의 개념** 설명 및 python실습 ( 이전에 학습한 선형대수 개념이 어떻게 적용되는지 확인해보면서 설명할 것)

6\. **PCA가 가지는 의의 및 이점**, **한계**

#### **linear projection (정사영)**

'물체의 그림자를 어떤 물체 위에 비추는 일 또는 그 비친 그림자', '도형이나 입체를 다른 평면에 옮기는 일'

한 벡터가 다른 벡터에 수직으로 투영하는 것

#### **Linear Tranformation (**선형 변환**)**

선형변환은 두 벡터 공간 사이의 함수

ex) 좌표 평면에 벡터 하나가 있다고 가정할 때, 그 벡터를 확대하거나 축소하거나 회전시키거나

반사하는 것 모두 '변환'이라 생각할 수 있다. 단순히 기존 행렬의 변형이라고 생각할 뿐만 아니라 기존행렬을 

다른 좌표 공간으로 이동시킨다고 생각할 수도 있다.

ex) 전기 및 음성신호로 부터의 소음 여과와 컴퓨터 그래픽 등에 **사용**

<img src="https://blog.kakaocdn.net/dn/mahak/btrhZMKNpxU/jtUkvSaHvhcvNvvaJfVjWK/img.gif">

---

## **고유값과 고유벡터** 

고윳값 : 특성이라는 뜻이다. 여기서 특성이란 방향은 변하지 않고 크기만 변하는 특성을 말한다.

고유벡터 : 즉 고유벡터란 벡터에 선형 변환을 취했을 때, 방향은 변하지 않고 크기만 변하는 벡터를 말한다.

그리고 선형 변환 이후 변한 크기가 고윳값을 의미한다.

정리하면 고유 벡터란 어떤 벡터에 선형 변환을 취했을  때, 방향은 변하지 않고, 크기만 바뀌는 벡터를 의미하고

고유값이란 고유 벡터가 변환 되는 '크기'정도를 의미한다.

<img src="https://blog.kakaocdn.net/dn/boJXUP/btrh6IB34TO/t7iltrrE9wBKfaavdAXv3K/img.jpg">

<img src="https://blog.kakaocdn.net/dn/cjBESD/btrh93SXgzl/1hY8awuWyWnM71qMnF5Jj0/img.gif">

어떤 변환에대해 고유한 특성이라 하면 전과 후의 방향이 유지되는 것을 의미한다.

물론 + - 가능하다. 늘리기의 반대는 압축이기 때문인데

이 고유한 특성을 지닌채 변환되는 벡터를 고유벡터(Eigen vector)

그 변환의 비례정도를 고유값(Eigen value)라고 한다.

<img src="https://blog.kakaocdn.net/dn/bhN6Hy/btrh8DU1oXO/Ru4eMipd58p2BsJwvrJsFK/img.png">

이 선형 변환에서 수평축은 그대로 수평축으로 남기 때문에

파란색 화살표는 방향이 변하지 않지만 빨간색 화살표는 방향이 변하게 된다.

따라서 파란색 화살표는 이 변환의 고유벡터가 되고 빨간색 화살표는 고유벡터가 아니다. 

또한 파란색 화살표의 크기가 변하지 않았으므로 이 벡터의 고유값은 1이다.

#### **어디에 쓰일까?**

-   **주성분 분석 (PCA)**  
    고차원의 데이터를 정보의 손실을 최소화하며 저차원의 데이터로 변환시키는 방법  
    차원의 저주가 이미지를 다루기 위해 고전적인 컴퓨터 비전에서 매우 중요한 문제였으며 기계 학습에서도  
    차원이 높은 모델의 특징 그 결과 학습에 많은 양의 데이터가 필요하다.
-   **특이값분해 (SVD)**
-   **스펙트럼 클러스터링 (Spectrum Clustering)**  
    행렬의 고유 벡터를 사용하여 K 군집을 찾는 방법.  
    이러한 문제를 처리하고 클러스터링을 위한 다른 알고리즘을 쉽게 능가한다.
-   **컴퓨터 비전의 관심 지점 감지**  
    컴퓨터 비전에서 이미지의 관심 지점은 해당지역에서 고유한 지점이다.  
    이러한 점은 기능으로 사용되는 클래식 컴퓨터비전에서 중요한 역할을 한다.  
    
    ---
    

#### **고차원의 문제**

-   고차원은 많은 공간을 가지고 있기 때문에 매우 희박할 위험이 있음
-   훈련 세트의 차원이 클수록 과대 적합 위험이 커지며, 새로 들어온 샘플도 떨어져 있을 확률이 높아  
    예측이 불안정함.
-   훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우거나, 꼭 필요한 feature만 선정해서  
    차원을 감소시켜 해결 가능  
    핵심은 주어진 데이터의 정보 손실을 최소화 하면서 줄인다는 것.  
    즉, 수 많은 feature중 가장 중요한 feature몇 개 만을 선택하는 것이 PCA이다.

#### **차원축소를 하는 목적과 이점**

-   **시각화 (Visualization)**  
    위에서 설명한 것처럼 3차원이 넘어간 시각화는 우리 눈으로 볼 수 없다.  
    따라서 차원 축소를 통해 시각화를 하고, 시각화를 통해 데이터 패턴을 쉽게 인지할 수 있다.
-   **노이즈 제거 (Reduce Noise)**  
    쓸모없는 feature를 제거함으로써 노이즈를 제거할 수 있다.
-   **메모리 절약 (Preserve useful info in low memory)**  
    쓸모없는 feature를 제거하면 메모리도 절약된다.
-   **퍼포먼스 향상**  
    불필요한 feature들을 제거해 모델 성능 향상에 기여를 한다.  
    

#### **차원의 저주**

머신러닝에서 데이터 셋의 특성(feature)가 많아지면, 각 특성인 하나의 차원(dimension) 또한 증가하게 된다.

이렇게 데이터의 차원이 증가할 수록 데이터 공간의 부피가 기하 급수적으로 증가하기 때문에,

데이터의 밀도는 차원이 증가할 수록 희소(sparse)해진다.

아래의 그림은 데이터의 차원이 증가할수록 각 영역(region)에 포함되는 데이터의 개수를 나타낸 그림이다.

그림에서 알 수 있듯이 차원이 증가할 수록 데이터의 밀도가 희소해지는것을 알 수 있다.

<img src="https://blog.kakaocdn.net/dn/dmJX6s/btrh95i1K7W/rVOsMQw4HMTXGDcVVhA68K/img.png">

#### **일반적으로 feature를 줄이는 방법 3가지**

-   EDA를 통해 Null 값이 너무 많은 데이터나 모든 값이 거의 동일한 컬럼을 삭제
-   상관계수를 확인하여 너무 낮은 상관계수 컬럼을 삭제
-   모델을 사용하여 차원을 낮추는것

## **주성분 분석 (Principal Component Analysis).**

차원 축소 방법중 하나로서 주성분 분석은 여러 피쳐가 통계적으로 서로 상관관계가 없도록 변환시키는 방법이다.

주성분 분석은 오직 공분산 행렬에만 영향을 받는다.

또한 피쳐 간 상관관계를 기반으로 테이터의 특성을 파악한다.

먼저 데이터셋의 공분산 행렬의 고윳값과 고유 벡터를 구한다.

이때 고유값은 고유 벡터의 크기를 나타내며 분산의 크기를 의하느고 고유 벡터는 분산의 방향을 의미한다.

분산이 큰 고유벡터에 기존 데이터를 투영해 새로운 데이터를 구할 수 있는데 이렇게 구한 벡터를 주성분 벡터라고 부른다.

차원 축소를 하기 전 기존 데이터의 공분산 행렬을 구한다면 피처 사이에 서로 상관관계가 존재할 확률이 높다.

하지만 주성분 분석을 통해 차원 축소를 한 후의 주성분 벡터들은 서로 상관관계가 없다.

왜냐하면 주성분 벡터가 서로 직교하며, 주성분 벡터가 직교한다는 것은 서로 상관관계가 없다는 것을

의미하기 때문이다.

#### **주성분 분석 알고리즘**

1.  데이터 셋 준비
2.  데이터 셋을 피처별로 표준화 (평균 빼고 분산으로 나누기)
3.  피처 데이터의 공분산 행렬 구하기
4.  3번에서 구한 공분산 행렬의 고유값, 고유벡터 구하기
5.  고유값을 큰 순서대로 내림차순 정렬
6.  d차원으로 줄이고 싶은 경우. 크기 순서대로 d개의 고유값 선정
7.  6에서 선정한 고유값에 대응하는 고유벡터로 새로운 행렬 생성
8.  1에서 준비한 오리지널 데이터를 7에서 만든 새로운 공간으로 투영

#### **여기서 고유값과 고유벡터의 의미란?**

-   고유벡터  
    각 피처의 분산 방향을 알 수 있다.
-   고유값  
    분산의 크기를 알 수 있다.

즉, 데이터가 여러 방향으로 흩어져 있을때, 고유 벡터를 이용하면 각 흩어짐에 대한 방향을 파악할 수 있으며

고유값을 이용하면 어느 정도 흩어져 있는지 그 크기를 알 수 있다.

#### **차원축소 방법중 투영이란?**

투영 (Projection)

일반적으로 대부분의 실제 데이터셋에서는 모든 데이터의 특성, 즉 차원이 고르게 분포되어 있지 않다. 필기체 숫자 데이터셋인 MNIST를 예로들면, 어떤 특성(각 pixel을 하나의 특성으로 볼 때)은 거의 변화가 없고, 또 어떤 특성은 다른 특성들과 서로 연관되어 있다.

이렇듯 학습 데이터셋은 고차원 공간에서 저차원 **부분 공간(subspace)** 에 위치하게 된다. 즉, 고차원의 데이터의 특성 중 일부 특성으로 데이터를 표현할 수 있다는 말이 된다.

아래의 그림은 왼쪽 3차원 공간상의 데이터를 2차원 부분 공간으로 투영(porjection)시켜 2차원 데이터셋으로 만든 것이다.

<img src="https://blog.kakaocdn.net/dn/bopegb/btrhZM6oE9R/hnktv1JGRJRqafxS0Dbax1/img.png">

  
  

#### **차원축소의 단점**

-   일부 정보를 소실하여 훈련 알고리즘의 성능을 감소 시킬 수 있음
-   계산 비용이 높음  
    ex) SVD를 사용하면 행렬 계산이 많아져서 계산 비용이 높았고 이를 극복하기 위한 점진적 PCA를 사용
-   머신러닝 파이프라인의 복잡도를 증가시킴
-   변환된 데이터를 이해하기 어려움 (PC1, PC2)

출처 : [https://wegonnamakeit.tistory.com/31](https://wegonnamakeit.tistory.com/31)

       [https://brunch.co.kr/@chris-song/104](https://brunch.co.kr/@chris-song/104)

       [https://m.blog.naver.com/spin898/221146786482](https://m.blog.naver.com/spin898/221146786482)  
       [https://hycentureon.tistory.com/1](https://hycentureon.tistory.com/1)

       [https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-9-PCA-Principal-Components-Analysis](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-9-PCA-Principal-Components-Analysis)
