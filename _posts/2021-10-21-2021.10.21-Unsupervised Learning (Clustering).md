학습목표 : 비지도 학습 중 클러스터링에 대해 학습하기-머신러닝 전초단계

학습정리기준 :

1\. 머신러닝의 분류 및 종류 살펴보기

2\. 비지도 학습 중 클러스터링에 대한 개념 및 목적 알아볼 것

3\. 2번과 pca를 연계하여 다양한 클러스터링 알고리즘 중 K-means clustering에 대한 개념학습 및 적용분야 찾아볼 것 (python실습)

4 k-means clustering을 제외하고 많이 활용되는 클러스터링 알고리즘 하나를 선택하여 개념학습 및 적용분야 찾아볼 것 (python 실습)

<img src="https://blog.kakaocdn.net/dn/cZoMwX/btrivDsrGuv/7vGkTwmHAZEJjZU9Wk6es0/img.png">

**비지도학습** : 기계에게 데이터에 대한 통찰력을 부과하는 것

데이터의 성격을 파악하거나 데이터를 정리정돈하는데 쓰임 (관찰을 통해 의미나 관계를 밝혀냄)

## **군집화 (Clustering)**

#### ****Clustering 이**란?**

관측된 여러 개의 변수값들로부터 유사성에 기초하여 n개의 클러스터 또는 비슷하 샘플의 그룹으로 할당하는 방법

#### **Clustering vs Classification**

-   군집화 (Clustering)  
    어떤 대상들을 구분해서 그룹을 만드는 것  
    비지도학습, 각 개체의 Label을 사전에 알 수 없는 경우 사용  
    정확도는 상대적으로 낮은 편이다.
-   분류 (Classification)  
    어떤 대상이 어떤 그룹에 속하는지 판단하는 것  
    지도학습, 각 개체의 Label을 사전에 알 수 있을 때 사용  
    정확도는 상대적으로 높은 편이다.
-   공통점으로 각 샘플은 하나의 그룹에 할당한다는 것 !

#### **Hard Clustering vs Soft Clustering**

-   Hard Clustering   
    각 샘플을 가장 가까운 하나의 클러스터에 할당하는 방식  
    클러스터의 크기가 많이 다른 경우 결과가 좋지 않을 수 있음
-   Soft Clustering  
    샘플이 각 클러스터에 할당될 수 있는 점수를 부여하는 방식  
    점수 : 샘플과 각 센트로이드 사이의 거리 (L2 Norm), 유사도 점수 같은 것이 쓰일 수 있음

#### ****Clustering** 의 활용**

-   고객분류  
    고객을 구매 이력이나 행동 등을 기반으로 군집으로 모아 동일 군집 내의 고객에게 상이한 마케팅 전략 적용
-   데이터 분석  
    상권별 스마트폰 판매 예측 시 상권의 특성을 기준으로 선 군집 후 주력 판매 제품 선정 및 수요예측
-   이미지 분할  
    색을 기반으로 픽셀을 클러스터로 모은 후, 클러스터를 평균 색으로 바꾸어 물체의 윤곽 감지  
    ex) 자율주행 사물 인식센서 알고리즘으로 사용될 수 있음
-   이상치 탐지  
    모든 클러스터에 친화성이 낮은 샘플을 이상치로 간주  
    ex) 법인카드 이상 거래 탐지를 위해 정상 카드 결제 내역들로 군집 형성 후 이 군집들에 속하지 않는 사용 시 탐지
-   준지도 학습  
    라벨링된 샘플이 적을시 라벨이 있는 데이터와 없는 데이터 모두 섞고 선 군집 수행 후  
    동일한 군집에 있는 모든 샘플에 라벨 전파
-   차원축소
-   검색엔진

## **K-means Clustering**

주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터 내 샘플 간 응집도를 최대로 하는 방식으로 동작,  
최적의 K값을 찾아야 한다.

<img src="https://blog.kakaocdn.net/dn/8VH5K/btrio190gfm/mJtostlAYkZsvx0kKpPph1/img.png">

#### **K-means Process**

1.  k개 Centroid선택   
    초기 군집 중심으로 k개의 객체를 임의로 선택
2.  할당  
    자료를 가장 가까운 군집 중심(Centroid)에 할당
3.  중심 갱신  
    각 군집 내의 자료들의 평균을 계산하여 Centroid를 갱신
4.  반복  
    Centroid의 변화가 거의 없을 때 (or max\_iter)까지 2-3단계 반복

주의해야 할 점은 어떻게 최적의 초기 Cluster 개수 (k값)을 설정하냐다.  
또한 Centroid를 처음 어떻게 두느냐에 따라서 최종성능이 많이 흔들릴 수 있다

<img src="https://blog.kakaocdn.net/dn/CcSSe/btrivPNdPto/I8QYzCTg4MKrD3mGjV1Ke1/img.gif">

#### **초기값 설정방법과 구현시 고려사항**

-   **Hierarchical Clustering**   
    완전 밑바닥부터 끝까지 bottom-up 방식으로 모두 계층화시킨다.
-   **Elbow-method  
    **여러 후보 k값들 중 적절함을 평가하고 적절한 초기값을 선정한다.  
    이때 적절함을 평가하는 척도는 예를들어 군집 중심으로부터 거리를 계산하는 등 다양한 방법 중 선택한다.  
    이 방법은 Grid-search 처럼 일일이 시행하면서 최적의 k의 초기값을 설정한다.
-   **Information Criterion Approach  
    **클러스터링 모델에 대해 likelihood(조건부확률)을 점수로 계산한다.  
    K-means의 경우에는 **Gaussian Mixture model**을 활용해 likelihood를 계산한다.
-   **Rule of thumb  
    **간단한 공식을 따른다. 공식은 바로 하단의 그림이다. 이 때 n은 데이터의 개수이다.  
    <img src="https://blog.kakaocdn.net/dn/dhQMvU/btritNbK3dM/F1XNIg6Xdvdf2QCsckSIz1/img.png">  
    K-means ++ 초기화 알고리즘
-   가지고 있는 데이터 Point중 Random하게 1개를 선택하여 첫 번째 Centroid로 지정
-   나머지 데이터 Point들에 대해 첫 번째 Centroid까지의 거리를 계산
-   두 번째 Centroid는 첫 번째 Centroid로부터 최대한 먼 곳에 배치된 데이터 Point로 지정
-   Centroid가 k개가 될 때까지 2-3반복

계산 복잡도

\- K-means의 계산 복잡도는 일반적으로는 샘플 개수, 클러스터 개수, Feature 개수에 선형적으로 증가

\- Clustering이 잘 되지 않는 경우에는 계산 복잡도가 급격히 증가할 수 있음

\- 최적의 솔루션을 찾기 위해 실행할 알고리즘 반복횟수를 크게 줄일 수 있기 때문에 똑똑한 초기화 단계에

드는 추가계산이 충분한 가치가 있다고 판단됨.

#### **K-means 의 한계**

-   장점  
    \- 속도가 빠르고 확장이 용이함
-   단점   
    \- 최적이 아닌 솔루션을 피하기 위해 알고리즘을 여러 번 실행해야함  
    \- 클러스터의 개수를 지정해야 함  
    \- 초기 Centroid의 위치에 따라 최적의 군집이 나오지 않을 수 있음  
    \- 클러스터의 크기나 밀집도가 서로 다르거나 원형이 아닐 경우 잘 작동하지 않음  
    \- 이상치에 취약함  
    \- 모든 데이터를 거리로만 판단하게 됨으로, 사전에 주어진 목적이 없어 결과 해석이 어렵다는 단점 존재
-   특징  
    \- 데이터에 따라서 잘 수행할 수 있는 군집 알고리즘이 다름  
    \- 실행하기 전 입력 특성의 스케일을 맞추어야 함  
    \- 대안으로 K-medoids 알고리즘을 통해 클러스터 줌심을 평균값이 아닌 '데이터'하나를 선택해 그 '데이터'를 중심을 가정할 수있다

#### **K-means 의 활용예시**

-   데이터 분류,클러스터링 방법
-   성향이 불분명한 시장 분석
-   트렌드와 같이 명확하지 못한 기준 분석
-   패턴인식, 음석인식 기본 기술
-   관련성을 알 수 없는 데이터 초기 분류

## **DBSCAN Clustering**

데이터의 위치 정보를 이용해서 클러스터링하는 기법.

밀집된 연속적 지역을 클러스터로 정의

클러스터의 갯수를 미리 지정하지 않는다.

병합 군집이나 K-means 보다는 다소 느리지만 비교적 큰 데이터셋에도 적용

DBSCAN은 특성 공간에서 가까이 있는 데이터가 많아 붐비는 지역의 포인트를 찾음

이런 지역을 **밀집 지역****dense region**이라 함

밀집 지역에 있는 포인트를 **핵심 포인트****core point**라고함

#### **DBSCAN Clustering vs K-means Clustering** 

-   K-means Clustering  
    평균과의 거리가 얼마나 떨어졌는지를 결정
-   DBSCAN Clustering  
    데이터의 밀도를 활용해서 클러스터링을 한다.

<img src="https://blog.kakaocdn.net/dn/XPtpJ/btripGq4zDt/cvLM5RG4eKb6zD8spe2pd0/img.png">

#### **DBSCAN Parameter**

<img src="https://blog.kakaocdn.net/dn/xHoww/btritNCT0Ok/WLNx3dREptKKhhZUWAHAd1/img.png">

-   Core point  
    한 데이터 벡터로 부터 거리 E 내에 위치한 다른 데이터 벡터들의 수가 N보다 클 경우  
    클러스터링을 하는데, 이때 중심이 되는 벡터이다.
-   Border point  
    핵심 벡터로부터 거리 E 내에 위치해서 같은 클러스터로 분류되며 자체로는 핵심벡터가 아니고  
    해당 클러스터의 외곽을 형성하는 벡터이다.
-   Noise point  
    어떠한 클러스터에도 속하지 않는 벡터

#### **DBSCAN Process**

1\. 무작위로 데이터 포인트를 선택

2\. 그 포인트에서 eps 거리안의 모든 포인트를 찾음

2-1 eps 거리 안에 있는 데이터 포인트 수가 min\_samples보다 적다면 어떤 클래스에도 속하지 않는 잡음noise로 레이블

2-2 eps 거리 안에 있는 데이터 포인트 수가 min\_samples보다 많으면 핵심 포인트로 레이블하고 새로운 클러스터 레이블할당

3\. 2-2의 핵심 포인트의 eps거리안의 모든 이웃을 살핌

3-1 만약 어떤 클러스터에도 아직 할당되지 않았다면 바로 전에 만든 클러스터 레이블을 할당

3-2 만약 핵심 포인트면 그 포인트의 이웃을 차례로 확인

4\. eps 거리안에 더이상 핵심 포인트가 없을 때까지 진행

#### **DBSCAN 의 장점과 단점**

-   장점  
    \- 클러스터 수를 지정할 필요가 없으며, 알고리즘이 자동으로 클러스터의 수를 찾는다.  
    \- 원 모양의 클러스터뿐만 아니라, 불특정한 모양의 클러스터도 찾을 수 있다.  
    \- 클러스터가 채택할 수 있는 모양과 크기에는 높은 유연성이 있다.  
    \- 클러스터링을 수행하는 동시에 노이즈 데이터도 분류할 수 있기 때문에 이상치에 의해   
      클러스터링 성능이 하락하는 현상을 완화할 수 있다.
-   단점  
    \- 다양한 밀도의 클러스터를 잘 찾지 못함  
    \- 데이터가 입력되는 순서에 따라 클러스터링 결과가 변한다.  
    \- 알고리즘이 이용하는 거리 측정 방법에 따라 클러스터링 결과가 변한다.  
    \- 데이터의 특성을 모를 경우에는 알고리즘의 적절한 hyper-parameter를 설정하기가 어렵다.

마지막으로 

K-means Clustering , 계층적 클러스터링 , DBSCAN은 데이터 간 유사도를 위해 유클리드 거리를 적용하여 계산,

유클리드 거리를 이용하는 기법에는 '차원의 저주'라는 효과가 발생한다는 단점이 존재한다.

차원의 저주는 머신러닝,데이터 마이닝 분야에서 사용하는 용어인데 데이터의 차원이 증가하면 해당 공간의

부피가 기하급수적으로 증가하게 되고, 모델을 추정하기 위해 필요한 샘플 데이터의 개수도 기하급수적으로

증가한다. 또한 공간 상에 분포하는 데이터의 밀도 역시 매우 희박하게 된다. 이런 현상을 차원의 저주라는 

표현을 써서 어려움을 나타낸다,

출처 : [https://techblog-history-younghunjo1.tistory.com/83](https://techblog-history-younghunjo1.tistory.com/83)

       [https://muzukphysics.tistory.com/entry/ML-13-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-k-means-Clustering-%ED%8A%B9%EC%A7%95-%EC%9E%A5%EB%8B%A8%EC%A0%90-%EC%A0%81%EC%9A%A9-%EC%98%88%EC%8B%9C-%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5](https://muzukphysics.tistory.com/entry/ML-13-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-k-means-Clustering-%ED%8A%B9%EC%A7%95-%EC%9E%A5%EB%8B%A8%EC%A0%90-%EC%A0%81%EC%9A%A9-%EC%98%88%EC%8B%9C-%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5)  
       https://m.blog.naver.com/samsjang/221023672149
