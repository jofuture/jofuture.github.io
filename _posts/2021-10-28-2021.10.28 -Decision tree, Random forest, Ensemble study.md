학습정리기준:

1\. **결정트리, 랜덤포레스트, 앙상블** 키워드 위주로 개념을 공부하기

2\. 결정트리의 **장/단점, 사용하는 이유**에 대해서 알아보기

3\. 결정트리에서 확인 가능한 **특성중요도**가 무엇인지 알아보기

4\. **결정트리모델과 선형모델과 어떤 차이**가 있는지 알아보기

5\. **결정트리와 과적합**에 대해서 공부해보기

6\. 결정트리와 랜덤포레스트를 코드구현

## **결정트리 개념**

**"Decision Tree란, 데이터들이 가진 속성들로부터 분할 기준 속성을 판별하고, 분할 기준 속성에 따라**

**트리 형태로 모델링하는 분류 예측 모델"**

나무 모형의 성장 과정은 x들로 이루어진 입력 공간을 재귀적으로 분할하는 과정

<img src="https://blog.kakaocdn.net/dn/cP3MWU/btri0CIuT1b/PjFuldscy6RTqprnIxYu70/img.png">

## **결정트리 알고리즘 (CART)**

-   Classification And Regression Tree의 약자
-   각 독립변수를 이분화 하는 과정을 반복하여 이진 트리 형태를 형성함으로써 분류를 수행하는 방법
-   Greedy Algorithm으로 최적의 솔류션을 보장하지는 않지만, 매 단계에서 최적의 분할을 찾는 과정을 반복
-   불순도의 척도로 종속변수 (y)가 이산형일 경우는 지니계수를 이용,  
    연속형일 경우는 분산의 감소가 최소화되는 방향으로 분리
-   sklearn에서 사용하는 알고리즘 

**※ 목표 변수에 따른 분리기준**

| 목표 변수 | 기준 값 |
| --- | --- |
| 이산형 | 카이제곱 통계량의 p값, 지니 지수, 엔트로피 지수 |
| 연속형 | 분산 분석에서 F-통계량, 분산의 감소량 |

## **매개변수 규제**

| **Hyper Parameter** | **설명** |
| --- | --- |
| min\_samples\_split | 분할되기 위해 노드가 가져야 하는 최소 샘플 수 |
| min\_samples\_leaf | 리프 노드가 가지고 있어야 할 최소 샘플 수 |
| max\_features | 최적의 분할을 위해 고려할 최대 Feature수 |
| max\_depth | 트리의 최대 깊이. 지정하지 않을 경우 완벽하게 나누어 질 때까지 계속 성장 |
| max\_leaf\_nodes | 리프 노드의 최대 개수 |
| min\_impurity\_decrease | 분할로 얻어질 최소한의 불순도 감소량 |

## **Pruning**

-   결정트리는 Train전에 파라미터 수가 결정되지 않는 '비파라미터 모델'
-   모델 구조가 데이터에 맞춰져서 고정되지 않고 자유도가 높아 Overfitting의 위험이 높은 편
-   오버피팅이 심해진다. 이런 오버피팅을 막고자 사후 혹은 사전 가지치기를 수행할 수 있다.

1.  사전 가지치기  
    \- 더 이상 분리가 일어나지 않고 현재의 마디가 끝마디가 되도록 하는 규칙  
    \- 의사결정나무의 깊이를 지정하거나, 끝마디의 레코드 수의 최소 개수를 지정  
    \- 'min'으로 시작하는 하이퍼파라미터를 증가시키거나,  
      'max'로 시작하는 하이퍼파라미터를 감소시켜 규제
2.  사후 가지치기  
    \- 의사결정나무를 Full Tree로 만든 후 적절한 수준에서 Terminal Node를 결합해 주는 것  
    \- 마디에 속하는 자료가 일정 수 이하힐 때 분할을 멈추고, 비용-복잡도 가지치기를 활용하여   
      성장시킨 나무에 대한 가지치기 수행(ccp\_alpha 매개변수 활용)

## **결정트리의 활용과 장 단점**

-   Segmentation  
    데이터를 비슷한 특성을 갖는 몇개의 그룹으로 분할하여 그룹별 특성을 발견하고자 하는 경우 활용
-   Classification  
    여러 예측변수 (x)들에 근거하여 관측 개체의 목표 변수 (y) 범주를 몇 개의 등급으로 분류하고자 하는 경우에 활용
-   Regression  
    자료에서 규칙을 찾아내고 이를 이용해서 미래의 사건을 예측하고자 하는 경우 활용
-   Demension Reduction & Feature Selection  
    매우 많은 수의 예측 변수 중에서 목표 변수에 큰 영향을 미치는 변수들을 구분하고자 하는 경우에 활용
-   Effect of Interaction  
    여러 개의 예측 변수들을 결합해서 목표 변수에 작용하는 규칙을 파악하고자 하는 경우 활용  
    범주의 병합 또는 연속형 변수의 이산화에 활용: 범주형 목표 변수의 범주를 소수의 몇 개로 병합하거나  
    연속형 목표변수를 몇 개의 등급으로 이산화하고자 하는 경우 활용  
    \-> 이 관점에서 모델링 초반에 EDA 느낌으로 활용하기에 좋다.

-   **결정트리의 장점**  
-   
    \- 해석의 용이성 : 나무 구조로부터 어떤 입력 변수가 목표 변수를 설명하기 위해서 더 중요한지를 쉽게 파악 가능 (White box 모델), 설명력이 아주 뛰어나고 사람의 의사결정과 닮았다.  
      
    \- 상호작용 효과 해석 가능 : 두 개 이상의변수가 결합(비선형성) 하여 목표 변수에 어떻게 영향을 주는지 쉽게 파악 가능  
      
    \- 비모수적 모형 : 선형성, 정규성, 등분산성 등의 가정을 필요로 하지 않음  
      순서형 또는 연속형 변수는 단지 순위만 분석에 영향을 주기 때문에 이상값에 민감하지 않음   
      -> Scaling등의 데이터 전처리가 거의 필요하지 않음  
      
    \- 높은 유연성과 정확도 : 설명 변수나 목표 변수에 수치형 변수와 범주형 변수를 모두 사용 가능  
    \- 시각화 하기 좋다.  
      
      
    
-   **결정트리의 단점** 
      
    \- 비연속성 : 연속형 변수를 비연속적인 값으로 취급하기 때문에 분리의 경계점 근방에서는  
      예측 오류가 클 가능성이 있음  
      
    \- 선형성 결여 : 선형모형에서는 다른 예측 변수와 관련 시키지 않고서도 독립적으로 각 변수의 영향력을   
      해석할 수 있으나, 트리에서는 불가능하다는 한계  
      
    \- 비안정성 : Training Data에만 의존하는 의사결정 나무는 새로운 자료의 예측에서는 불안정하여 Overfitting이   
      일어날 가능성이 있음( Prunning을 해도 완벽하게 해결x )  
      
    \- 강인하지 못하다(Non-robust).input data의 작은 변화에도 최종 예측값은 크게 변화할 수 있다  
      
    
-   여러 개의 decision tree를 종합하는 방법을 이용하여 극복 가능 !
-   꽤 단순한 모델이기 때문에 장점도 많지만 단점도 있다. 때문에 결정트리만 가지고 만든 모델은 실제로   
    예측값으로 활용되기는 어렵다. 

## **결정 트리 분석 특성 중요도**

**특성 중요도(Feature Importance)**

전체 트리를 살펴보는 것은 어려울 수 있으니, 트리가 어떻게 작동하는지 요약하는 속성들을 사용할 수 있다.

특성 중요도도 그 중의 하나로, 트리를 만드는 결정에 각 특성이 얼마나 중요한지를 평가하는 것이다.

이 값은 0과 1사이의 숫자로, 각 특성에 대해 0은 전혀 사용되지 않았다는 뜻이고 1은 완벽하게 타겟 클래스를 예측했다는 뜻이다.

특성 중요도 전체의 합은 1이다. 즉, 특성 중요도가 높으면 트리 구조에서 높은 층에 해당하는 노드의 테스트(질문)라는 것이고 특성 중요도가 낮으면 낮은 층에 해당하는, 세세하게 클래스를 분류하는 노드의 테스트(질문)이거나 혹은 테스트에 사용되지 않는 특성이라는 것이다.

 - 그러나 특성 중요도가 낮다고 하여 그 특성이 유용하지 않다는 것은 아니다. 단지 트리가 그 특성을 선택하지 않았을 뿐이며, 다른 특성이 동일한 정보를 지니고있어서일 수 있다. 음수가 될 수 있고 특성의 중요도를 나타내는 선형 모델의 가중치(계수)와는 달리, 특성 중요도는 항상 양수이며 특성이 어떤 클래스를 지지하는지는 알 수 없다. 

 - 회귀 결정 트리에서도 이런 특성은 동일하게 적용되나 분류와는 달리 회귀를 위한 트리 모델에는 중요한 속성이 하나 있다.

모든 트리 기반 회귀 모델은 외삽(Extrapolation), 훈련 데이터의 범위 밖에 위치하 포인트에 대해서는 예측을 할 수 없다는 것이다. 

선형 모델은 전체 데이터의 경향성을 토대로 모델을 만들기 때문에 훈련 데이터의 범위 밖이더라도 예측할 수 있지만, 과는 달리, 훈련 데이터가 위치한 영역을 분할하여 예측하는 결정 트리는 범위 밖에 위치한 데이터에 대해서는 영역이 분할되어 있지 않기 때문에 모델이 예측을 할 수 없는 것이다.

## **Bootstrap** 

주어진 자료에서 단순 랜덤 복원 추출 방법을 활용하여 동일한 크기의 표본을 여러 개 생성하는 샘플링 방법

---

## **랜덤 포레스트**

랜덤 포레스트는 기본적으로 조금씩 다른 여러 결정 트리의 묶음으로, 훈련 과정에서 구성한 다수의 결정 트리로부터 분류 또는 회귀 분석 결과를 출력함으로써 동작한다. 

분산이 크다는 최대 단점을 가진 Decision tree를 보완한 모델. 배깅기법 활용

랜덤 포레스트의 아이디어는 각각의 단일 결정 트리는 비교적 예측은 잘 하지만, 데이터 일부에 과대적합(Overfitting)되는 경향을 가진다는데 기초한다. 즉, 작동 자체는 잘하므로 서로 다른 방향으로 과대적합된 트리를 많이 만들고 앙상블시켜 그 결과르 평균낸다면 과대적합된 양을 줄일 수 있다는 것이다. 이렇게 하면 트리 모델의 예측 성능은 유지되면서 과대적합이 줄어들 수 있다는 것이 증명되었다. 

-   방법 : 2가지에 무작위성을 주어 약한 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기 생성  
    1\. Random하게 Data Sampling  
    2\. Random하게 Feature Sampling

-   **랜덤 포레스트의 장점  
    **1\. 분산을 줄여 배깅보다 더 좋은 예측력을 보임  
    2\. 랜덤 포레스트 내 모든 트리에 걸쳐서 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 상대적 중요도를 측정 -> Feature Selection시 참조 가능  
    3\. 훈련 데이터에 과대적합되는 경향이 있는 단일 결정 트리 학습의 단점을 회피할 수 있다.   
      
    
-   **랜덤 포레스트의 단점**  
    1\. 이론적 설명이나 최종 결과에 대한 해석이 어려움  
    2\. 계산하는 데에 오랜 시간이 걸림  
      
    
-   **Extremely Randomized Trees (Extra-trees)**  
    1\. Sampling된 Feature들을 사용해 무작위로 분할한 다음 그 중 최상의 분할을 선택  
    2\. 일반적인 랜덤 포레스트보다 훨씬 빠름  
    \-> 랜덤포레스트처럼 편향이 늘어나는 리스크를 갖고 있지만 모든 노드에서 특성마다 가장 최적의 임계값을 찾는것이 트리 알고리즘에서 가장 시간이 많이 소요되는 작업중 하나 뭐가 성능이 더 좋은지는 알 수x

---

## **앙상블**

**"앙상블 알고리즘이란, 주어진 자료로부터 여러 개의 예측 모형을 만든 후에 예측 모형들을 조합하여 하나의 최종 예측 모형을 만드는 방법" -> 의사결정 트리의 오버피팅 한계를 극복하기 위한 전략**

앙상블 학습의 핵심은 여러 개의 약 분류기 (Weak Classifier)를 결합하여 강 분류기(Strong Classifier)를 만드는 것.

그리하여 모델의 정확성이 향상된다.

앙상블 학습법에는 배깅(Bagging)과 부스팅(Boosting) 두 가지가 있다.

## **배깅(Bagging)**

배깅은 샘플을 여러 번 뽑아(Bootstrap) 각 모델을 학습시켜 결과물을 집계(Aggregration)하는 방법.

-   Bootstrap Aggregating의 줄임말로, 학습 데이터의 중복을 허용하며 학습 데이터 세트를 나누어 모델링한 후  
    결합하여 최종 예측 모형을 만드는 알고리즘
-   중복을 허용하기 때문에 특정 학습 데이터는 중복으로 사용되고 특정 학습 데이터는  
    사용되지 않아 편향될 가능성이 있음  
    
-   강점 : 평균 예측 모형을 구하여 분산을 감소 시킴 -> 서브셋에 다양성을 증가시키기 때문  
    
-   단점 : 계산 복잡도는 다소 높음
-   oob\_score란?

**페이스팅 (Pasting)**

-   학습 데이터를 중복하여 사용하지 않고 세트를 나누어 사용하는 기법 (비복원 추출 방식 )
-   배깅에 비해 성능이 떨어지는 경우가 많음

<img src="https://blog.kakaocdn.net/dn/dsr7lW/btri7QS7lTB/wVQjk36XSDKYOLU5JQOPy0/img.png">

## **부스팅(Boosting)**

**"Boosting이란, 잘못 분류된 개체들에 대해 높은 가중치를 적용하여 앞의 모델을 보완해 가는 일련의 과정들을 반복해 최종 모형을 만드는 알고리즘"**

부스팅은 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법입니다. 배깅은 Deicison Tree1과 Decision Tree2가 서로 독립적으로 결과를 예측합니다. 여러 개의 독립적인 결정 트리가 각각 값을 예측한 뒤, 그 결과 값을 집계해 최종 결과 값을 예측하는 방식입니다. 하지만 부스팅은 모델 간 팀워크가 이루어집니다. 처음 모델이 예측을 하면 그 예측 결과에 따라 데이터에 가중치가 부여되고, 부여된 가중치가 다음 모델에 영향을 줍니다. 잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만드는 단계를 반복한다.

<img src="https://blog.kakaocdn.net/dn/B86wa/btri7RqYo00/CmJfUQTu3mW8uZm3Zz9Vd0/img.png">
     
---

## **결정트리모델과 선형모델**

어느 모델이 더 나은가? 그것은 가지고 있는 문제에 따라 다르다. 설명변수들과 반응변수 사이의 관계가 선형모델의 의해 잘 근사된다면 선형회귀와 같은 기법들이 잘 동작할 가능성이 높고 선형적 구조를 이용하지 않는 회귀트리와 같은 방법보다 성능이 나을 것이다. 반면에, 설명변수들과 반응변수 사이의 관계가 상당히 비선형적이고 복잡하다면 의사결정트리가 고전적 기법들보다 더 나을 수 있다. 아래 그림에서 한 예를 도시적으로 보여준다. 트리 기반의 기법과 고전적 기법의 상대적 성능은 교차검증 또는 검증셋 기법을 사용하여 검정오차를 추정함으로써 평가할 수 있다.

출처 : 핸즈온 머신러닝

        https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-11-   %EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning-%EB%B0%B0%EA%B9%85Bagging%EA%B3%BC-%EB%B6%80%EC%8A%A4%ED%8C%85Boosting

        https://kolikim.tistory.com/23?category=733478

        https://kolikim.tistory.com/22
