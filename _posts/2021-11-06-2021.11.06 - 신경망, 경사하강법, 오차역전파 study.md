학습목표:

신경망과 퍼셉트론의 개념 학습 - 구조, 입출력, 학습역전파 (Backpropagation, backward pass)

학습방법에 대한 구체적인 지식

## **퍼셉트론**

가장 단순한 형태의 신경망

Hidden Layer가 없이 Single Layer로 구성됨. 입력 피처들과 가중치,Activation,출력 값으로 구성

<img src="https://blog.kakaocdn.net/dn/bbx5DG/btrj3NBvyTC/y47903rFD5KigUpIMD1nF1/img.png">

## **MLP의 개념과 딥러닝의 차이점**

<img src="https://blog.kakaocdn.net/dn/r1xyZ/btrj3hCRh61/QBZlOZk4fjwxjEYGKp7qA1/img.jpg">

퍼셉트론의 단점은 매개변수 중 가중치를 수동으로 사람이 설정해야 한다.

신경망은 수동으로 설정해야 하는 가중치를 데이터로부터

적절한 값을 학습하는 능력을 가지는데 이는 신경망의 대표 성질이다.

퍼셉트론과 신경망의 가장 큰 차이는 활성화 함수 이다.

활성화 함수의 주요 목적은 딥러닝 네트웍에 비해 비선형성을 적용하기 위함이다.

퍼셉트론는 계단 함수로 신경망은 시그모이드 함수로 활성화 함수를 사용하는데, 최근에는 ReLU 함수도 사용하며 신경망 모델마다 활성화 함수를 선택하여 사용한다.

두 활성화 함수인 시그모이드 함수와 계단 함수의 차이점은 연속적인 실수를 출력하는가? 안하는가에 있다.

공통점은 입력 값이 커질 수록 1, 입력 값이 작아질수록 0으로 출력한다는 것이다.  
  

이 밖에도 둘 다 비선형 함수이며 신경망의 활성화 함수는 비선형 함수여야 한다.

선형 함수라면 신경망으로 표현하는 의미가 없어진다.

Relu : 은닉층에서 사용됨

Sigmoid : 이진분류시 마지막 Classification 출력층에 사용

Softmax : 멀티분류시 마지막 Classification 출력층에 사용

## **신경망의 기본구조**

<img src="https://blog.kakaocdn.net/dn/brFQWA/btrj7yiYnaw/nD6QaE0OKwD4xk7HKupXPk/img.png">

### 입력층 (Input Layers)

-   입력층은 데이터셋으로부터 입력을 받는다.
-   입력 변수의 수와 입력 노드의 수는 같다.
-   보통 입력층은 어떤 계산도 수행하지 않고 그냥 값들을 전달하기만 하는 특징을 가지고 있다.
-   신경망의 층수(깊이, depth)를 셀 때 입력층은 포함하지 않는다.
-   위의 그림의 신경망은 2층이라고 할 수 있다.

### 은닉층 (Hidden Layers)

-   계산이 일어나는 층이 둘 이상인 신경망을 다층(multilayer) 신경망 이라고 부른다.
-   perceptron에서 이름을 빌려와서, multilayer perceptron (MLP)라고 부르기도 한다.
-   계산이 없는 입력층과 마지막 출력층 사이에 있는 층들을 은닉층(Hidden Layers) 이라고 부른다.
-   은닉층에 있는 계산의 결과를 사용자가 볼 수 없기(hidden) 때문에 이런 이름이 붙었다.
-   딥러닝(deep learning)은 사실 두 개 이상의 (이때 부터 깊다(deep)라고 합니다) 은닉층들을 가진 신경망, 입력층을 제외하고 세보면, 3개 이상의 Layer를 갖는 신경망을 의미한다.

### 출력층 (Output Layers)

-   신경망 가장 오른쪽, 마지막 층이다.
-   출력층에는 대부분 활성함수(activation function)가 존재하는데 활성화함수는 풀고자 하는 문제에 따라 다른 종류를 사용한다.  
    \-회귀 문제에서 예측할 목표 변수가 실수값인 경우, 활성화함수가 필요하지 않으며 출력노드의 수는 출력변수의 갯수와 같다.
-   이진 분류(binary classification) 문제의 경우, 시그모이드(sigmoid) 함수를 사용해서 출력을 확률 값으로 변환하여 클래스(Class or label)를 결정하도록 한다.
-   다중클래스(multi-class)를 분류하는 경우, 출력층 노드가 부류 수 만큼 존재하며 소프트맥스(softmax) 함수를 활성화 함수로 사용한다.
-   복잡한 딥러닝 방법론들에서는 은닉층에서도 활성함수를 사용하기 시작한다.

### 신경망의 Nonlinearities(비선형성)

신경망 학습은 데이터에서 필요한 특성들을 신경망이 알아서 조합하여 찾아낸다.  
즉, 최소한의 데이터에 대한 전처리는 해야 하지만 심화된 특성 공학(Feature Engineering)을 사용해 특성들을 찾아낼 필요는 없다는 것이다.

깊은 신경망, 즉 딥러닝과 머신러닝의 차이는 **표현학습(representation learning)**에 있다.  
딥러닝은 데이터 특성(feature)을 우리가 풀고자 하는 문제를 풀기 쉽도록 표현(representation)하도록 학습하는 능력이 있다. 신경망의 구조와 깊이를 변화시키며 데이터를 더욱 유용하게 표현할 수 있다.

## **Backpropagation(오차역전파)**

인공신경망을 학습 시키기 위한 알고리즘중 하나

내가 뽑고자 하는 타겟값과 모델이 계산한 아웃풋값이 얼마나 차이가 있는지

계산한 후에 그 오차값을 다시 뒤로 전파해 나가면서 각 노드가 가진 가중치값들을 업데이트 시키는 과정

<img src="https://blog.kakaocdn.net/dn/cNKEtV/btrj7Q44CZh/yTZKQ4BGjVPthxBOG8QWE0/img.png">


출처: [https://velog.io/@joo4438/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%A1%B0](https://velog.io/@joo4438/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EA%B5%AC%EC%A1%B0)
https://liveyourit.tistory.com/63
